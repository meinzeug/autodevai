name: Performance Monitoring

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 */6 * * *' # Every 6 hours
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Benchmark type'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - frontend
          - backend
          - e2e
          - load

env:
  NODE_VERSION: '22'
  PERFORMANCE_PORT: '50001'
  METRICS_PORT: '50002'

jobs:
  frontend-performance:
    name: Frontend Performance Analysis
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Build for performance testing
        run: npm run build

      - name: Install Lighthouse CI
        run: npm install -g @lhci/cli@0.12.x

      - name: Start preview server
        run: |
          npm run preview -- --port ${{ env.PERFORMANCE_PORT }} &
          sleep 30
        env:
          PORT: ${{ env.PERFORMANCE_PORT }}

      - name: Run Lighthouse CI
        run: |
          lhci collect \
            --url=http://localhost:${{ env.PERFORMANCE_PORT }} \
            --numberOfRuns=3 \
            --settings.chromeFlags="--no-sandbox --headless"

      - name: Bundle size analysis
        run: |
          npm run analyze:bundle || echo "Bundle analysis tool not found"

          # Create bundle size report
          echo "# Bundle Size Report" > bundle-report.md
          echo "Generated: $(date)" >> bundle-report.md
          echo "" >> bundle-report.md

          # Analyze dist folder
          if [ -d "dist" ]; then
            echo "## Build Output" >> bundle-report.md
            du -sh dist/* >> bundle-report.md
            echo "" >> bundle-report.md
            
            # Find large files
            echo "## Large Files (>100KB)" >> bundle-report.md
            find dist -type f -size +100k -exec ls -lh {} \; | awk '{print $5, $9}' >> bundle-report.md
          fi

      - name: Performance budget check
        run: |
          # Check if critical files exceed size limits
          MAX_JS_SIZE=500000  # 500KB
          MAX_CSS_SIZE=100000 # 100KB

          echo "Checking performance budgets..."

          for js_file in dist/assets/*.js; do
            if [ -f "$js_file" ]; then
              size=$(stat -c%s "$js_file")
              if [ $size -gt $MAX_JS_SIZE ]; then
                echo "::warning::JavaScript file $js_file ($size bytes) exceeds budget ($MAX_JS_SIZE bytes)"
              fi
            fi
          done

          for css_file in dist/assets/*.css; do
            if [ -f "$css_file" ]; then
              size=$(stat -c%s "$css_file")
              if [ $size -gt $MAX_CSS_SIZE ]; then
                echo "::warning::CSS file $css_file ($size bytes) exceeds budget ($MAX_CSS_SIZE bytes)"
              fi
            fi
          done

      - name: Upload Lighthouse reports
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-reports
          path: |
            .lighthouseci/
            bundle-report.md
          retention-days: 30

  backend-performance:
    name: Backend Performance Testing
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            src-tauri/target
          key: ${{ runner.os }}-cargo-perf-${{ hashFiles('src-tauri/Cargo.lock') }}

      - name: Build backend with optimizations
        working-directory: src-tauri
        run: cargo build --release

      - name: Install performance testing tools
        run: |
          cargo install criterion-table
          sudo apt-get update
          sudo apt-get install -y hyperfine

      - name: Run Rust benchmarks
        working-directory: src-tauri
        run: |
          # Run criterion benchmarks if available
          cargo bench --bench performance_benchmarks || echo "No benchmarks found"

      - name: API performance testing
        run: |
          # Start the Tauri backend for API testing
          cd src-tauri && ./target/release/autodev-ai &
          BACKEND_PID=$!
          sleep 10

          # Basic API performance tests
          echo "Running API performance tests..."

          # Test health endpoint
          hyperfine --warmup 3 --runs 10 \
            "curl -s http://localhost:50003/health" \
            --export-json api-perf.json || echo "API tests skipped"

          # Cleanup
          kill $BACKEND_PID || true

      - name: Memory usage analysis
        working-directory: src-tauri
        run: |
          # Check binary size
          echo "# Memory & Size Analysis" > memory-report.md
          echo "Generated: $(date)" >> memory-report.md
          echo "" >> memory-report.md

          echo "## Binary Size" >> memory-report.md
          ls -lh target/release/autodev-ai >> memory-report.md
          echo "" >> memory-report.md

          # Static analysis
          echo "## Dependencies Analysis" >> memory-report.md
          cargo tree --depth 1 >> memory-report.md

      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        with:
          name: backend-performance
          path: |
            api-perf.json
            src-tauri/memory-report.md
          retention-days: 30

  e2e-performance:
    name: End-to-End Performance Testing
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install dependencies
        run: npm install

      - name: Install Playwright
        run: npx playwright install chromium

      - name: Build application
        run: npm run build

      - name: Build Tauri backend
        working-directory: src-tauri
        run: cargo build --release

      - name: Start full application
        run: |
          # Start backend
          cd src-tauri && ./target/release/autodev-ai &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV

          # Start frontend
          npm run preview -- --port ${{ env.PERFORMANCE_PORT }} &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV

          sleep 30

      - name: Run E2E performance tests
        run: |
          # Run Playwright tests with performance metrics
          npx playwright test --config=playwright.config.ts --project=chromium \
            --reporter=json > e2e-results.json || true

      - name: Analyze E2E performance
        run: |
          echo "# E2E Performance Report" > e2e-report.md
          echo "Generated: $(date)" >> e2e-report.md
          echo "" >> e2e-report.md

          # Extract performance metrics from Playwright results
          if [ -f e2e-results.json ]; then
            echo "## Test Results" >> e2e-report.md
            jq -r '.suites[].specs[] | "- \(.title): \(.ok)"' e2e-results.json >> e2e-report.md || echo "No detailed results available" >> e2e-report.md
          fi

      - name: Cleanup processes
        if: always()
        run: |
          kill $BACKEND_PID || true
          kill $FRONTEND_PID || true

      - name: Upload E2E reports
        uses: actions/upload-artifact@v4
        with:
          name: e2e-performance
          path: |
            e2e-results.json
            e2e-report.md
          retention-days: 30

  load-testing:
    name: Load Testing
    runs-on: ubuntu-22.04
    if: github.event_name == 'schedule' || github.event.inputs.benchmark_type == 'load' || github.event.inputs.benchmark_type == 'all'
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Install dependencies
        run: npm install

      - name: Install load testing tools
        run: |
          npm install -g artillery@latest
          sudo apt-get install -y apache2-utils

      - name: Build and start application
        run: |
          npm run build
          cd src-tauri && cargo build --release

          # Start services
          cd src-tauri && ./target/release/autodev-ai &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV

          npm run preview -- --port ${{ env.PERFORMANCE_PORT }} &
          FRONTEND_PID=$!
          echo "FRONTEND_PID=$FRONTEND_PID" >> $GITHUB_ENV

          sleep 30

      - name: Create load testing configuration
        run: |
          cat > load-test.yml << EOF
          config:
            target: 'http://localhost:${{ env.PERFORMANCE_PORT }}'
            phases:
              - duration: 60
                arrivalRate: 5
                name: "Warm up"
              - duration: 120
                arrivalRate: 10
                name: "Ramp up load"
              - duration: 300
                arrivalRate: 15
                name: "Sustained load"
          scenarios:
            - name: "Homepage load"
              weight: 70
              flow:
                - get:
                    url: "/"
            - name: "API health check"
              weight: 30
              flow:
                - get:
                    url: "/health"
          EOF

      - name: Run load tests
        run: |
          echo "Starting load test..."
          artillery run load-test.yml --output load-results.json || echo "Load test completed with warnings"

      - name: Generate load test report
        run: |
          artillery report load-results.json --output load-report.html || echo "Report generation failed"

          # Create summary report
          echo "# Load Testing Report" > load-summary.md
          echo "Generated: $(date)" >> load-summary.md
          echo "" >> load-summary.md

          if [ -f load-results.json ]; then
            echo "## Summary Statistics" >> load-summary.md
            jq -r '.aggregate | 
              "- Total Requests: \(.counters."http.requests" // "N/A")",
              "- Success Rate: \(.rates."http.request_rate" // "N/A")",
              "- Average Response Time: \(.latency.mean // "N/A")ms",
              "- 95th Percentile: \(.latency.p95 // "N/A")ms"' load-results.json >> load-summary.md || echo "Failed to parse results" >> load-summary.md
          fi

      - name: Apache Bench additional testing
        run: |
          echo "" >> load-summary.md
          echo "## Apache Bench Results" >> load-summary.md
          ab -n 1000 -c 10 http://localhost:${{ env.PERFORMANCE_PORT }}/ > ab-results.txt 2>&1 || true

          # Extract key metrics
          if [ -f ab-results.txt ]; then
            grep -E "(Requests per second|Time per request|Transfer rate)" ab-results.txt >> load-summary.md || echo "Apache Bench results not available" >> load-summary.md
          fi

      - name: Cleanup processes
        if: always()
        run: |
          kill $BACKEND_PID || true
          kill $FRONTEND_PID || true

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load-results.json
            load-report.html
            load-summary.md
            ab-results.txt
          retention-days: 30

  performance-regression-check:
    name: Performance Regression Analysis
    runs-on: ubuntu-22.04
    needs: [frontend-performance, backend-performance, e2e-performance]
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Download current performance data
        uses: actions/download-artifact@v4

      - name: Download baseline performance data
        continue-on-error: true
        run: |
          # Download performance data from main branch
          curl -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -L "https://api.github.com/repos/${{ github.repository }}/actions/artifacts" \
            -o artifacts.json

          # Extract latest performance artifacts from main branch
          # This is a simplified example - in practice, you'd want more robust artifact retrieval

      - name: Analyze performance changes
        run: |
          echo "# Performance Regression Analysis" > regression-report.md
          echo "Generated: $(date)" >> regression-report.md
          echo "" >> regression-report.md

          # Compare Lighthouse scores
          if [ -f lighthouse-reports/.lighthouseci/lighthouseci-results.json ]; then
            echo "## Lighthouse Score Changes" >> regression-report.md
            
            # Extract current scores
            CURRENT_PERF=$(jq -r '.results[0].lhr.categories.performance.score // "N/A"' lighthouse-reports/.lighthouseci/lighthouseci-results.json)
            CURRENT_ACCESS=$(jq -r '.results[0].lhr.categories.accessibility.score // "N/A"' lighthouse-reports/.lighthouseci/lighthouseci-results.json)
            CURRENT_SEO=$(jq -r '.results[0].lhr.categories.seo.score // "N/A"' lighthouse-reports/.lighthouseci/lighthouseci-results.json)
            
            echo "- Performance: $CURRENT_PERF" >> regression-report.md
            echo "- Accessibility: $CURRENT_ACCESS" >> regression-report.md
            echo "- SEO: $CURRENT_SEO" >> regression-report.md
            
            # Set thresholds for regression
            if (( $(echo "$CURRENT_PERF < 0.9" | bc -l) )); then
              echo "::warning::Performance score below threshold (0.9): $CURRENT_PERF"
            fi
          fi

          echo "" >> regression-report.md
          echo "## Bundle Size Changes" >> regression-report.md
          if [ -f lighthouse-reports/bundle-report.md ]; then
            tail -10 lighthouse-reports/bundle-report.md >> regression-report.md
          fi

      - name: Comment on PR
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('regression-report.md')) {
              const report = fs.readFileSync('regression-report.md', 'utf8');
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## 📊 Performance Analysis\n\n' + report
              });
            }

  performance-alerts:
    name: Performance Alerting
    runs-on: ubuntu-22.04
    needs: [frontend-performance, backend-performance, load-testing]
    if: always() && (github.event_name == 'schedule' || github.event_name == 'push')
    steps:
      - name: Download performance data
        uses: actions/download-artifact@v4

      - name: Check performance thresholds
        run: |
          ALERT_NEEDED=false
          ALERT_MESSAGE=""

          # Check Lighthouse performance score
          if [ -f lighthouse-reports/.lighthouseci/lighthouseci-results.json ]; then
            PERF_SCORE=$(jq -r '.results[0].lhr.categories.performance.score // 0' lighthouse-reports/.lighthouseci/lighthouseci-results.json)
            if (( $(echo "$PERF_SCORE < 0.8" | bc -l) )); then
              ALERT_NEEDED=true
              ALERT_MESSAGE="🚨 Performance score dropped to $PERF_SCORE"
            fi
          fi

          # Check for large bundle sizes
          if [ -d "dist" ]; then
            TOTAL_SIZE=$(du -sb dist | cut -f1)
            MAX_SIZE=5242880  # 5MB
            if [ $TOTAL_SIZE -gt $MAX_SIZE ]; then
              ALERT_NEEDED=true
              ALERT_MESSAGE="$ALERT_MESSAGE\n📦 Bundle size exceeded 5MB: $(echo $TOTAL_SIZE | numfmt --to=iec)"
            fi
          fi

          echo "ALERT_NEEDED=$ALERT_NEEDED" >> $GITHUB_ENV
          echo "ALERT_MESSAGE<<EOF" >> $GITHUB_ENV
          echo -e "$ALERT_MESSAGE" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

      - name: Send performance alert
        if: env.ALERT_NEEDED == 'true'
        uses: 8398a7/action-slack@v3
        with:
          status: warning
          channel: '#performance'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          text: |
            ⚠️ Performance Alert - AutoDev-AI

            ${{ env.ALERT_MESSAGE }}

            Commit: ${{ github.sha }}
            Branch: ${{ github.ref_name }}

            Please review the performance metrics and take corrective action.
