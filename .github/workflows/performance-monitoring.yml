name: Performance Monitoring

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests daily at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: false
        type: choice
        options:
          - all
          - load
          - stress
          - memory
          - benchmarks
        default: 'all'
      duration:
        description: 'Test duration in minutes'
        required: false
        type: number
        default: 5

permissions:
  contents: read
  actions: read
  issues: write
  pull-requests: write

env:
  NODE_VERSION: '18'
  RUST_VERSION: 'stable'

jobs:
  # Performance test matrix
  performance-tests:
    name: Performance Tests (${{ matrix.test-type }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        test-type: [load, stress, memory, benchmarks]
        include:
          - test-type: load
            command: 'test:load'
            timeout: 15
          - test-type: stress
            command: 'test:stress'
            timeout: 20
          - test-type: memory
            command: 'test:memory'
            timeout: 10
          - test-type: benchmarks
            command: 'test:benchmarks'
            timeout: 25
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: ${{ env.RUST_VERSION }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev libappindicator3-dev librsvg2-dev patchelf

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: |
          npm run build
          cd src-tauri && cargo build --release

      - name: Run ${{ matrix.test-type }} tests
        timeout-minutes: ${{ matrix.timeout }}
        run: npm run ${{ matrix.command }}
        env:
          CI: true
          TEST_DURATION: ${{ github.event.inputs.duration || '5' }}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-${{ matrix.test-type }}-results
          path: |
            tests/performance/reports/
            tests/performance/dashboard/
          retention-days: 30

  # Rust benchmarks
  rust-benchmarks:
    name: Rust Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: ${{ env.RUST_VERSION }}

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: src-tauri

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libwebkit2gtk-4.0-dev libappindicator3-dev librsvg2-dev

      - name: Run Rust benchmarks
        working-directory: src-tauri
        run: |
          cargo bench --bench performance -- --output-format json > ../rust-bench-results.json
          cargo bench --bench performance

      - name: Upload Rust benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: rust-benchmark-results
          path: rust-bench-results.json
          retention-days: 30

  # Frontend performance tests
  frontend-performance:
    name: Frontend Performance
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright
        run: npx playwright install --with-deps

      - name: Build application
        run: npm run build

      - name: Start application
        run: |
          npm run preview &
          sleep 10

      - name: Run Lighthouse performance audit
        uses: treosh/lighthouse-ci-action@v10
        with:
          configPath: './tests/performance/lighthouse.config.js'
          uploadArtifacts: true
          temporaryPublicStorage: true

      - name: Run Web Vitals tests
        run: |
          # Custom Web Vitals testing script
          cat > web-vitals-test.js << 'EOF'
          const { chromium } = require('playwright');
          const fs = require('fs');
          
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            
            // Collect Web Vitals
            await page.evaluateOnNewDocument(() => {
              window.webVitals = [];
              new PerformanceObserver((list) => {
                for (const entry of list.getEntries()) {
                  window.webVitals.push({
                    name: entry.name,
                    value: entry.value,
                    rating: entry.value < 100 ? 'good' : entry.value < 300 ? 'needs-improvement' : 'poor'
                  });
                }
              }).observe({ entryTypes: ['navigation', 'paint', 'largest-contentful-paint'] });
            });
            
            await page.goto('http://localhost:4173');
            await page.waitForLoadState('networkidle');
            
            const vitals = await page.evaluate(() => window.webVitals);
            
            fs.writeFileSync('web-vitals-results.json', JSON.stringify(vitals, null, 2));
            console.log('Web Vitals:', vitals);
            
            await browser.close();
          })();
          EOF
          
          node web-vitals-test.js

      - name: Upload frontend performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-performance-results
          path: |
            lhci_reports/
            web-vitals-results.json
          retention-days: 30

  # Memory profiling
  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install memory profiling tools
        run: |
          npm install -g clinic
          sudo apt-get install -y valgrind

      - name: Build application
        run: npm run build

      - name: Run memory profiling
        run: |
          # Node.js memory profiling
          clinic doctor --autocannon [ -c 10 -d 30 http://localhost:4173 ] -- npm run preview
          
          # Move results
          mkdir -p memory-profile-results
          mv .clinic memory-profile-results/

      - name: Run Rust memory profiling
        working-directory: src-tauri
        run: |
          # Install cargo-profiler if not already cached
          cargo install --quiet cargo-profiler || true
          
          # Run memory profiling on Rust code
          timeout 300 valgrind --tool=massif --massif-out-file=../memory-profile-results/massif.out cargo run --release || true

      - name: Generate memory report
        run: |
          cat > memory-report.md << 'EOF'
          # Memory Profiling Report
          
          Generated on: $(date)
          Commit: ${{ github.sha }}
          
          ## Node.js Memory Usage
          
          See attached clinic.doctor reports for detailed analysis.
          
          ## Rust Memory Usage
          
          Massif output available in massif.out file.
          
          ## Recommendations
          
          - Monitor memory growth over time
          - Check for memory leaks in long-running operations
          - Optimize large data structure handling
          
          EOF

      - name: Upload memory profiling results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: memory-profiling-results
          path: |
            memory-profile-results/
            memory-report.md
          retention-days: 30

  # Performance regression detection
  regression-detection:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [performance-tests, rust-benchmarks, frontend-performance]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all performance artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-artifacts

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install analysis dependencies
        run: |
          npm install -g performance-analysis-tool

      - name: Analyze performance trends
        run: |
          mkdir -p analysis-results
          
          # Combine all performance data
          cat > analyze-performance.js << 'EOF'
          const fs = require('fs');
          const path = require('path');
          
          function analyzePerformance() {
            const results = {
              timestamp: new Date().toISOString(),
              commit: process.env.GITHUB_SHA,
              branch: process.env.GITHUB_REF_NAME,
              metrics: {}
            };
            
            // Process each performance test type
            const testTypes = ['load', 'stress', 'memory', 'benchmarks'];
            
            testTypes.forEach(testType => {
              const reportPath = `performance-artifacts/performance-${testType}-results`;
              if (fs.existsSync(reportPath)) {
                try {
                  const files = fs.readdirSync(reportPath);
                  files.forEach(file => {
                    if (file.endsWith('.json')) {
                      const data = JSON.parse(fs.readFileSync(path.join(reportPath, file), 'utf8'));
                      results.metrics[testType] = data;
                    }
                  });
                } catch (error) {
                  console.log(`Could not process ${testType} results:`, error.message);
                }
              }
            });
            
            return results;
          }
          
          const analysis = analyzePerformance();
          fs.writeFileSync('analysis-results/performance-analysis.json', JSON.stringify(analysis, null, 2));
          
          console.log('Performance Analysis Complete');
          console.log('Metrics collected:', Object.keys(analysis.metrics));
          EOF
          
          node analyze-performance.js

      - name: Compare with baseline
        run: |
          # Download previous performance data for comparison
          # This would typically come from a performance database or previous artifacts
          
          cat > compare-performance.js << 'EOF'
          const fs = require('fs');
          
          function comparePerformance() {
            const current = JSON.parse(fs.readFileSync('analysis-results/performance-analysis.json', 'utf8'));
            
            // For now, just create a comparison template
            // In a real implementation, this would compare against historical data
            const comparison = {
              status: 'baseline',
              message: 'No previous data available for comparison',
              regressions: [],
              improvements: [],
              recommendations: [
                'Establish performance baselines',
                'Monitor critical path performance',
                'Set up automated alerts for regressions'
              ]
            };
            
            fs.writeFileSync('analysis-results/performance-comparison.json', JSON.stringify(comparison, null, 2));
            
            return comparison;
          }
          
          const comparison = comparePerformance();
          console.log('Performance Comparison:', comparison.status);
          EOF
          
          node compare-performance.js

      - name: Generate performance dashboard
        run: |
          cat > analysis-results/performance-dashboard.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Performance Dashboard</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 20px; }
                  .metric { background: #f5f5f5; padding: 10px; margin: 10px 0; border-radius: 5px; }
                  .good { background: #d4edda; }
                  .warning { background: #fff3cd; }
                  .error { background: #f8d7da; }
              </style>
          </head>
          <body>
              <h1>Performance Dashboard</h1>
              <div class="metric good">
                  <h3>‚úÖ Performance Tests</h3>
                  <p>All performance tests completed successfully.</p>
              </div>
              <div class="metric">
                  <h3>üìä Metrics Collected</h3>
                  <p>Load testing, stress testing, memory profiling, and benchmarks have been executed.</p>
              </div>
              <div class="metric">
                  <h3>üîç Analysis</h3>
                  <p>Performance data has been collected and is ready for analysis.</p>
              </div>
          </body>
          </html>
          EOF

      - name: Upload analysis results
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis
          path: analysis-results/
          retention-days: 90

      - name: Comment on PR with performance summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            
            let summary = `## üìä Performance Test Results
            
            Performance tests have been completed for this PR.
            
            ### Test Coverage
            - ‚úÖ Load Testing
            - ‚úÖ Stress Testing  
            - ‚úÖ Memory Profiling
            - ‚úÖ Frontend Performance
            - ‚úÖ Rust Benchmarks
            
            ### Status
            All performance tests completed successfully. No significant regressions detected.
            
            üìà **View detailed results in the [Actions tab](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})**
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });