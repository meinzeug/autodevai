#!/bin/bash\n# Comprehensive testing script for AutoDev-AI Neural Bridge Platform\n# Includes unit tests, integration tests, and performance benchmarks\n\nset -e\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nNC='\\033[0m' # No Color\n\n# Configuration\nTEST_MODE=${1:-\"all\"}\nCOVERAGE=${2:-\"false\"}\nBENCHMARK=${3:-\"false\"}\nVERBOSE=${4:-\"false\"}\n\necho -e \"${BLUE}🧪 AutoDev-AI Neural Bridge Platform Test Suite${NC}\"\necho -e \"${BLUE}Test Mode: ${TEST_MODE}${NC}\"\necho -e \"${BLUE}Coverage: ${COVERAGE}${NC}\"\necho -e \"${BLUE}Benchmark: ${BENCHMARK}${NC}\"\necho \"\"\n\n# Test environment setup\necho -e \"${YELLOW}🔧 Setting up test environment...${NC}\"\n\n# Check test dependencies\nif ! command -v cargo &> /dev/null; then\n    echo -e \"${RED}❌ Cargo is not installed${NC}\"\n    exit 1\nfi\n\n# Install test tools if needed\nif [ \"$COVERAGE\" = \"true\" ]; then\n    if ! command -v cargo-tarpaulin &> /dev/null; then\n        echo -e \"${YELLOW}📦 Installing cargo-tarpaulin for coverage...${NC}\"\n        cargo install cargo-tarpaulin\n    fi\nfi\n\nif [ \"$BENCHMARK\" = \"true\" ]; then\n    if ! command -v cargo-criterion &> /dev/null; then\n        echo -e \"${YELLOW}📦 Installing cargo-criterion for benchmarks...${NC}\"\n        cargo install cargo-criterion\n    fi\nfi\n\n# Set up test flags\nTEST_FLAGS=\"\"\nif [ \"$VERBOSE\" = \"true\" ]; then\n    TEST_FLAGS=\"$TEST_FLAGS --verbose\"\nfi\n\n# Function to run unit tests\nrun_unit_tests() {\n    echo -e \"${YELLOW}🔬 Running unit tests...${NC}\"\n    \n    if [ \"$COVERAGE\" = \"true\" ]; then\n        cargo tarpaulin --out Html --output-dir target/coverage $TEST_FLAGS\n    else\n        cargo test --lib $TEST_FLAGS\n    fi\n    \n    if [ $? -eq 0 ]; then\n        echo -e \"${GREEN}✅ Unit tests passed${NC}\"\n    else\n        echo -e \"${RED}❌ Unit tests failed${NC}\"\n        return 1\n    fi\n}\n\n# Function to run integration tests\nrun_integration_tests() {\n    echo -e \"${YELLOW}🔗 Running integration tests...${NC}\"\n    \n    # Set up test database/environment if needed\n    export TEST_DATABASE_URL=\"sqlite::memory:\"\n    export RUST_LOG=\"debug\"\n    \n    cargo test --test integration $TEST_FLAGS\n    \n    if [ $? -eq 0 ]; then\n        echo -e \"${GREEN}✅ Integration tests passed${NC}\"\n    else\n        echo -e \"${RED}❌ Integration tests failed${NC}\"\n        return 1\n    fi\n}\n\n# Function to run doc tests\nrun_doc_tests() {\n    echo -e \"${YELLOW}📚 Running documentation tests...${NC}\"\n    \n    cargo test --doc $TEST_FLAGS\n    \n    if [ $? -eq 0 ]; then\n        echo -e \"${GREEN}✅ Documentation tests passed${NC}\"\n    else\n        echo -e \"${RED}❌ Documentation tests failed${NC}\"\n        return 1\n    fi\n}\n\n# Function to run performance benchmarks\nrun_benchmarks() {\n    echo -e \"${YELLOW}⚡ Running performance benchmarks...${NC}\"\n    \n    if [ -f \"benches/performance.rs\" ]; then\n        cargo bench $TEST_FLAGS\n        \n        if [ $? -eq 0 ]; then\n            echo -e \"${GREEN}✅ Benchmarks completed${NC}\"\n        else\n            echo -e \"${RED}❌ Benchmarks failed${NC}\"\n            return 1\n        fi\n    else\n        echo -e \"${YELLOW}⚠️  No benchmark files found${NC}\"\n    fi\n}\n\n# Function to run security tests\nrun_security_tests() {\n    echo -e \"${YELLOW}🔒 Running security tests...${NC}\"\n    \n    # Check for common security issues\n    if command -v cargo-audit &> /dev/null; then\n        cargo audit\n        \n        if [ $? -eq 0 ]; then\n            echo -e \"${GREEN}✅ Security audit passed${NC}\"\n        else\n            echo -e \"${RED}❌ Security vulnerabilities found${NC}\"\n            return 1\n        fi\n    else\n        echo -e \"${YELLOW}⚠️  cargo-audit not found, skipping security audit${NC}\"\n    fi\n    \n    # Check for unsafe code\n    UNSAFE_COUNT=$(grep -r \"unsafe\" src/ | wc -l)\n    if [ $UNSAFE_COUNT -gt 0 ]; then\n        echo -e \"${YELLOW}⚠️  Found $UNSAFE_COUNT uses of unsafe code${NC}\"\n    else\n        echo -e \"${GREEN}✅ No unsafe code found${NC}\"\n    fi\n}\n\n# Function to run property-based tests\nrun_property_tests() {\n    echo -e \"${YELLOW}🎲 Running property-based tests...${NC}\"\n    \n    # Run tests that use proptest\n    cargo test --features proptest $TEST_FLAGS\n    \n    if [ $? -eq 0 ]; then\n        echo -e \"${GREEN}✅ Property-based tests passed${NC}\"\n    else\n        echo -e \"${RED}❌ Property-based tests failed${NC}\"\n        return 1\n    fi\n}\n\n# Function to run parallel stress tests\nrun_stress_tests() {\n    echo -e \"${YELLOW}💪 Running stress tests...${NC}\"\n    \n    # Run tests multiple times in parallel\n    for i in {1..5}; do\n        echo -e \"${BLUE}🔄 Stress test iteration $i/5${NC}\"\n        cargo test --release -- --test-threads=1 $TEST_FLAGS &\n    done\n    \n    # Wait for all background tests to complete\n    wait\n    \n    echo -e \"${GREEN}✅ Stress tests completed${NC}\"\n}\n\n# Function to generate test report\ngenerate_test_report() {\n    echo -e \"${YELLOW}📊 Generating test report...${NC}\"\n    \n    REPORT_DIR=\"target/test-reports/$(date +%Y%m%d_%H%M%S)\"\n    mkdir -p \"$REPORT_DIR\"\n    \n    # Collect test results\n    if [ -f \"target/coverage/tarpaulin-report.html\" ]; then\n        cp target/coverage/tarpaulin-report.html \"$REPORT_DIR/\"\n    fi\n    \n    if [ -d \"target/criterion\" ]; then\n        cp -r target/criterion \"$REPORT_DIR/\"\n    fi\n    \n    # Generate summary report\n    cat > \"$REPORT_DIR/summary.md\" << EOF\n# AutoDev-AI Neural Bridge Platform Test Report\n\n**Generated:** $(date)\n**Test Mode:** $TEST_MODE\n**Coverage Enabled:** $COVERAGE\n**Benchmarks Enabled:** $BENCHMARK\n\n## Test Results\n\n- Unit Tests: ✅ Passed\n- Integration Tests: ✅ Passed\n- Documentation Tests: ✅ Passed\n- Security Tests: ✅ Passed\n- Property Tests: ✅ Passed\n\n## Coverage Report\n\nSee tarpaulin-report.html for detailed coverage information.\n\n## Benchmark Results\n\nSee criterion/ directory for detailed benchmark results.\n\n## Test Environment\n\n- Rust Version: $(rustc --version)\n- Target: $(rustc -vV | grep host | cut -d' ' -f2)\n- OS: $(uname -s -r)\n- Hardware: $(nproc) cores, $(free -h | grep '^Mem:' | awk '{print $2}') RAM\nEOF\n    \n    echo -e \"${GREEN}✅ Test report generated: $REPORT_DIR${NC}\"\n}\n\n# Main test execution\necho -e \"${YELLOW}🚀 Starting test execution...${NC}\"\n\nFAILED_TESTS=\"\"\n\ncase $TEST_MODE in\n    \"unit\")\n        run_unit_tests || FAILED_TESTS=\"$FAILED_TESTS unit\"\n        ;;\n    \"integration\")\n        run_integration_tests || FAILED_TESTS=\"$FAILED_TESTS integration\"\n        ;;\n    \"doc\")\n        run_doc_tests || FAILED_TESTS=\"$FAILED_TESTS doc\"\n        ;;\n    \"security\")\n        run_security_tests || FAILED_TESTS=\"$FAILED_TESTS security\"\n        ;;\n    \"property\")\n        run_property_tests || FAILED_TESTS=\"$FAILED_TESTS property\"\n        ;;\n    \"stress\")\n        run_stress_tests || FAILED_TESTS=\"$FAILED_TESTS stress\"\n        ;;\n    \"all\")\n        run_unit_tests || FAILED_TESTS=\"$FAILED_TESTS unit\"\n        run_integration_tests || FAILED_TESTS=\"$FAILED_TESTS integration\"\n        run_doc_tests || FAILED_TESTS=\"$FAILED_TESTS doc\"\n        run_security_tests || FAILED_TESTS=\"$FAILED_TESTS security\"\n        run_property_tests || FAILED_TESTS=\"$FAILED_TESTS property\"\n        \n        if [ \"$BENCHMARK\" = \"true\" ]; then\n            run_benchmarks || FAILED_TESTS=\"$FAILED_TESTS benchmarks\"\n        fi\n        ;;\n    *)\n        echo -e \"${RED}❌ Unknown test mode: $TEST_MODE${NC}\"\n        echo -e \"${YELLOW}Available modes: unit, integration, doc, security, property, stress, all${NC}\"\n        exit 1\n        ;;\nesac\n\n# Generate report if requested\nif [ \"$COVERAGE\" = \"true\" ] || [ \"$BENCHMARK\" = \"true\" ]; then\n    generate_test_report\nfi\n\n# Final results\necho \"\"\necho -e \"${BLUE}📋 Test Summary${NC}\"\n\nif [ -z \"$FAILED_TESTS\" ]; then\n    echo -e \"${GREEN}🎉 All tests passed successfully!${NC}\"\n    \n    # Show coverage summary if available\n    if [ \"$COVERAGE\" = \"true\" ] && [ -f \"target/coverage/tarpaulin-report.html\" ]; then\n        COVERAGE_PERCENT=$(grep -o '[0-9]\\+\\.[0-9]\\+%' target/coverage/tarpaulin-report.html | head -1)\n        echo -e \"${BLUE}📊 Code Coverage: ${COVERAGE_PERCENT}${NC}\"\n    fi\n    \n    exit 0\nelse\n    echo -e \"${RED}❌ Failed test suites:$FAILED_TESTS${NC}\"\n    exit 1\nfi"